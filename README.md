# Krippendorff-s-Alpha
Krippendorff’s alpha is proposed as the standard reliability statistic for content analysis and similar data-making efforts. α counts pairs of categories or scale points that observers have assigned to individual units, treating observers as freely permutable and unaffected by their numbers. Krippendorff’s α defines a large family of reliability coefficients and embraces several known ones (Hayes and Krippendorff, 2007).
Analysts always aim for the highest reliability achievable, of course. As perfect reliability may be difficult to achieve, especially when coding tasks are complex and require elaborate cognitive processes, analysts need to know how much the data deviates from the ideal of perfect reliability and whether this deviation is above or below accepted reliability standards. These are the two main questions that any agreement measure should answer. a-agreement should not be confused with Cronbach's (1951) alpha, which is widely used in biometric and educational research for an entirely different purpose and is unsuitable for evaluating reliability in content analysis. In its most general form, a is defined by a = 1 -Do/De where Do is a measure of the observed disagreement and De is a measure of the disagreement that can be expected when chance prevails (Krippendorff, 2004).
